{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbee6b6-6a7e-4206-ba4c-b9b58dcac695",
   "metadata": {},
   "source": [
    "# Q-Learning: Advanced-Nim\n",
    "Hausarbeit Computational Intelligence - Michael Hopp\n",
    "\n",
    "Fachhochschule Erfurt - Wintersemester 2022/2023\n",
    "\n",
    "Prüfer: Prof. Dr. Frank Michael Dittes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dfe38-63ac-483f-8ff2-3f2d2488c9eb",
   "metadata": {},
   "source": [
    "## Aufgabenstellung\n",
    "Der Q-Learning-Algorithmus soll auf das Finden der optimalen Strategie für ein \"einfaches\" Spiel (hier Advanced-Nim) übertragen werden. Es soll möglich sein, die Q-Table einzusehen und interaktiv gegen einen Computergegner (AI) zu spielen.\n",
    "\n",
    "Als Zusatzaufgabe soll es möglich sein, die Konfiguration der Stapelanzahl und deren Stapelgrößen flexibel selbst zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d8638-6e00-4a1e-87ff-c7217dc53b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Einleitung\n",
    "Unter dem Begriff \"Nim-Spiel\" (engl. \"Nim-Game\") werden verschiedene Varianten von Spielen verstanden, bei denen durch **Zwei Spieler** abwechselnd aus einer Sammlung gleicher Gegenstände eine - je nach Regelwerk eingeschränkte - Anzahl von Gegenständen **aufgenommen**, und somit vom *Spielfeld* entfernt werden. Ziel der Spieler ist es, die verbleibende Anzahl der Gegenstände so zu manipulieren, dass der Gegenspieler keine andere Wahl hat, als den letzten verbleibenden Gegenstand aufzunehmen (Misére-Variante) oder selbst den letzten Gegenstand zu nehmen (Standardvariante).\n",
    "\n",
    "Diese Spiele unterliegen mathematischen Regeln, die optimale Lösungsstrategien ableiten lassen und zur Erstellung von Computer-Gegnern verwendet werden können (vgl. [Nim-Computer \"Nimatron\" 1940 und \"Nimrod\" 1951 für das 4-Reihen Spiel mit Gewinnstrategie nach Bouton](https://de.wikipedia.org/wiki/Nim-Spiel)).\n",
    "\n",
    "Der Q-Learning-Algorithmus soll dazu dienen, eine solche optimierte Lösungsstrategie zu finden. Dafür müssen zuerst die eigentlichen Spielregeln verstanden werden und eine angemessene Zustandskodierung gewählt werden, bevor das Programm dann implementiert wird. Die hier entstehende Ergebnisstrategie soll zum Vergleich gegen eine etablierte Gewinnstrategie antreten und das Verhalten der AI analysiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86c270-4e44-455c-bb3f-d184fa82e1bc",
   "metadata": {},
   "source": [
    "## Spielregeln\n",
    "Gespielt wird [Advanced Nim](https://www.youtube.com/watch?v=vt0ZddT_kBQ) wie es vom *Youtube Kanal Scam Nation* vorgestellt wird.\n",
    "\n",
    "Es spielen zwei Spieler gegeneinander. Vorzugsweise werden als Gegenstände Streichhölzer oder etwas ähnliches verwendet.\n",
    "\n",
    "Die Streichhölzer werden in insgesamt drei Stapeln mit drei, fünf und sieben Streichhölzern ausgelegt (damit ist die Anzahl der Streichhölzer für diese Spielvariante immer 15).\n",
    "\n",
    "**Spielaufbau:**\n",
    "``` \n",
    "3 III\n",
    "5 IIIII\n",
    "7 IIIIIII\n",
    "```\n",
    "Ein Spieler kann in seinem Zug **beliebig viele** Streichhölzer **aus einer Reihe** nehmen. \n",
    "\n",
    "Verloren hat der Spieler, der das letzte Streichholz aufnimmt (entspricht Misére-Variante, die durch Schadenfreude vermeintlich mehr Spaß macht)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e211b-2132-462a-92e0-814775d6f214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gewinnstrategie nach Scam Nation\n",
    "Wie könnte ein Mensch vorgehen, um zu gewinnen?\n",
    "Es existieren sogenannte **Pole Positions** (Schlüsselpositionen), die durch clevere Züge zu erzeugen sind, um dem Gegenspieler keine Möglichkeit auf einen Sieg zu lassen. Dadurch ist es nicht nötig, alle möglichen Folgezüge vorauszusehen (wie z.B. beim Schach), sondern die Strategie führt den Spieler von einer *sicheren Position* in die nächste.\n",
    "\n",
    "**Pole Positions** (Physische Ordnung irrelevant):\n",
    "```\n",
    "1 1 1\n",
    "1 2 3\n",
    "2 4 6\n",
    "1 4 5\n",
    "2 gleiche Stapel\n",
    "```\n",
    "\n",
    "Zur Verfolgung dieser Gewinnstrategie wird dem Gegenspieler der erste Zug überlassen. Anschließend wird versucht, als Gegenzug immer eine dieser Pole Positions zu erzeugen. Sobald nur noch wenige Streichhölzer liegen, gilt es, besonders aufmerksam zu sein, um den Siegeszug nicht zu verpassen oder durch die Anwendung der Strategie sogar zu verbauen (Beispiel: 1 1 0 als \"2 gleiche Stapel\" zu erzeugen, führt im Gegenzug zu 1 0 0 und damit zur eigenen Niederlage).\n",
    "\n",
    "**Schwachstelle:** Als **Konter** zu dieser Strategie kann der Gegenspieler im ersten Zug nur ein einziges Streichholz von einem beliebigen Stapel entnehmen, wodurch im zweiten Zug keine Pole Position erzeugt werden kann. Nach dem zweiten Zug kann somit der erste Spieler die Oberhand gewinnen und selbst eine Pole Position erzeugen, aus der sich der zweite Spieler nicht befreien kann - solange bei der Erzeugung der Positionen oder dem abschließenden Zug kein Fehler begangen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906833e6-8433-43f9-a234-943e2480e543",
   "metadata": {},
   "source": [
    "## Q-Learning Konzept\n",
    "Beim Q-Learning, welches zur Klasse des Reinforcement Learning gehört, wird die Qualität einer Aktion in einem Zustand ermittelt. Dabei muss lediglich der Wert (Reward) des Endzustandes festgelegt werden (*Gewonnen* ist gut und gibt 100 \"Punkte\", während *Verloren* schlecht ist und -100 \"Punkte\" gibt). Anhand dieses Endergebnisses wird beim Durchlaufen von möglichen Zustandsketten (festgelegt durch die \"Spielregeln\") allmählich die Q-Table mit weiteren Werten gefüllt. Die Q-Table gibt für jede mögliche Aktion jedes Zustandes einen Qualitätswert (Q-Value) an. Da zuerst nur der Endzustand bewertet ist, füllt sie sich \"von hinten\". Das ist vergleichbar mit einer menschlichen Strategie zum Lösen von Labyrinthen (die sich übrigens ebenfalls für Q-Learning anbieten): Dabei wird nicht etwa vom Start des Labyrinths ein Weg zum Ausgang gesucht, sondern umgekehrt vom Ausgang der Weg zum Start \"zurückverfolgt\".\n",
    "\n",
    "### Q-Value und Discount-Faktor\n",
    "Der Qualitätswert eines Zustandes wird aus dem sofortigen Reward, sowie dem im nächsten Zustand möglichen Reward berechnet. Anhand eines Discount-Faktors kann bestimmt werden, wie viel Wert auf zukünftige Rewards gelegt werden soll. Eine Analogie dazu: Ein Kind hat 2€ (Zustand) und könnte sich von diesem Geld ein Eis kaufen (Aktion). Der Reward wäre an dieser Stelle das Eis und da das Geld verbraucht ist, könnte nichts Weiteres gekauft werden. Nun könnte das Kind seine 2€ auch gewinnbringend investieren (Aktion). Diese Aktion würde keinen sofortigen Reward liefern. Allerdings besteht die Möglichkeit, dass das Kind einen Zustand erreicht, in dem es mehr als seine vorherigen 2€, zum Beispiel nämlich ganze 4€ zurückerlangt. Von diesem Zustand aus könnte sich das Kind *zwei* Portionen Eis kaufen (Aktion) und dementsprechend einen höheren Reward als zuvor erzielen. Welche Qualität die Aktionen haben und für welche sich das Kind schließlich entscheidet, hängt von dem Discount-Faktor ab, mit dem es die aufgeschobene Belohnung bewertet. Ist der Dicount-Faktor hoch, so fließen zukünftige Rewards stärker ein.\n",
    "\n",
    "Ein ähnliches Beispiel für den Effekt von Belohnungsaufschub und die Bereitschaft dazu liefert das [Stanford Marshmallow Experiment](https://en.wikipedia.org/wiki/Stanford_marshmallow_experiment).\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "Welche Aktion ist beim Lernen die *richtige*? Dafür gibt es verschiedene Strategien, die sich aus dem Verhältnis von Exploration zu Exploitation ergeben. Beim Durchlaufen der Zustandsketten könnten immer zufällige Aktionen oder stets die Aktion mit dem besten Q-Value gewählt werden:\n",
    "- Exploration: Die reine Zufallsvariante ist nicht sonderlich zielstrebig, dafür werden bei genügend häufiger Wiederholung alle Zustandsketten durchlaufen. Wenn die Anzahl der Zustände und der Aktionen hoch ist, muss die Anzahl der Wiederholungen entsprechend umfangreich sein. \n",
    "- Exploitation: Bei der Variante, die immer nur die bisher beste Aktion wählt, kann es dazu kommen, dass immer die gleichen Zustände durchlaufen werden, wodurch ggfs. bessere Aktionen nie probiert werden. Diese Variante findet keinen Weg aus einem lokalen Optimum heraus, falls einmal eines erreicht wird.\n",
    "\n",
    "Besser ist es, beide Verfahren zu kombinieren:\n",
    "- Prinzipiell kann die Aktion gewählt werden, die den besten Q-Value liefert, doch um die Chance zu haben, lokale Optima zu vermeiden, wird in sinnvollem Maße ein Zufallselement eingesetzt. Beispiel: In 10% der Fälle soll statt der besten Aktion eine beliebige (zulässige) Aktion gewählt werden.\n",
    "\n",
    "Ein Beispiel für Exploration und Exploitation aus dem Alltag wäre die Abwägung, in welchem Restaurant man essen gehen möchte: \n",
    "- Exploitation: Geht man zum Altbewährten, von dem man sicher weiß, wie gut es ist? \n",
    "- Exploration: Oder probiert man ein neues Lokal aus, das die Chance bietet, das neue Lieblingsrestaurant zu werden, sich jedoch auch als Reinfall herausstellen könnte?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a130e-093a-45af-8749-6addb2151dfd",
   "metadata": {},
   "source": [
    "## Implementierung\n",
    "Wie könnte ein Programm aussehen, das einer AI diese Variante von Nim beibringt?\n",
    "Essentiell ist dabei eine sinnvolle Zustandskodierung. Außerdem muss die Implementierung genügend Flexibilität bieten, um variable Stapelmengen und -größen zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9f779-acf9-414e-a77d-f57c9ec3c563",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vorüberlegungen\n",
    "Der wohl wichtigste Baustein beim Q-Learning-Algorithmus ist die Zustandskodierung. Neben den Spielregeln (Spielende und erlaubte Züge) muss dem Computer der aktuelle Spielzustand vermittelt werden, sodass er darauf das Q-Learning anwenden kann. Bevor also drauflosprogrammiert wird, sind die Dimensionen des Spiels zu erfassen und Möglichkeiten für die Zustandskodierung zu betrachten und abzuwägen.\n",
    "\n",
    "Die Anzahl der möglichen Zustände ergibt sich aus dem *Produkt der Stapelgrößen+1*. Beispiel für das Standardspiel [3 5 7]: ((3+1) * (5+1) * (7+1) = **192**)\n",
    "\n",
    "In jedem Zustand entspricht die Anzahl der möglichen Aktionen der *Summe der Stapelgrößen* (Summe der Hölzchen). Mit jedem Zug sinkt die Anzahl der möglichen Aktionen demnach um die entnommene Anzahl der Gegenstände.\n",
    "- Beispiel für Zustand [3 5 7]: 3+5+7 = 15\n",
    "- Beispiel für Zustand [0 0 2]: 0+0+2 = 2 (entweder zwei nehmen und verlieren oder nur eins nehmen und gewinnen)\n",
    "\n",
    "**Anforderung:** Erweiterbarkeit, um variable Stapelanzahl und beliebige Menge von Streichhölzern pro Stapel zu ermöglichen\n",
    "\n",
    "**Wunsch:** Die Konfiguration der Stapel und deren Hölzchenzahl (Stapelgröße) soll möglichst simpel für den Benutzer sein\n",
    "\n",
    "**Idee:** Zustände als Arrays aus Stapelgrößen. Stapelanzahl ergibt sich aus Indizes und eine Aktion umfasst einen Stapelindex, sowie die zu subtrahierende Anzahl von Hölzchen in diesem Array-Feld (Stapel). Mögliche Aktionen wären je Stapel eins bis maximal die Größe des Stapels (höchstens bis das Feld null ist).\n",
    "\n",
    "Zur Reduzierung der Zustands-Dubletten (z.B. 0,1 = 1,0), könnte das Array der Stapelgrößen nach jeder Anpassung sortiert werden.\n",
    "\n",
    "Alternativ könnten Zustände als Unsigned Integer mit den Stapeln über 10er-Potenzen kodiert werden. Beispiel: 357 als Startkonfiguration. Dadurch würden zur Dekodierung Mechanismen wie der Modulo-Operator benötigt werden. Der Vorteil wäre eine geringere Speicherbelastung. Die Sortierung zum Einsparen von Zustands-Dubletten würde jedoch umständlicher werden. Außerdem wären die Stapel somit auf maximal neun Objekte limitiert (single digits).\n",
    "\n",
    "Die Zustände könnten auch durchnummeriert werden, wobei die Kodierung (mit Überspringen von Zustands-Dubletten) nicht mehr einfach menschenlesbar ausfällt. Ausschnitt der Ganzzahlkodierung mit Durchnummerierung für das Standardspiel [3 5 7]:\n",
    "```\n",
    "State Code\n",
    "0,0,0 0\n",
    "0,0,1 1\n",
    "0,0,2 2\n",
    "...\n",
    "0,0,7 7\n",
    "0,1,1 8\n",
    "0,1,2 9\n",
    "...\n",
    "0,1,7 14\n",
    "0,2,3\n",
    "0,2,4\n",
    "...\n",
    "0,2,7\n",
    "...\n",
    "0,5,6\n",
    "0,5,7\n",
    "1,1,1\n",
    "1,1,2\n",
    "...\n",
    "1,1,7\n",
    "1,2,2\n",
    "...\n",
    "1,2,7\n",
    "1,3,3\n",
    "1,3,7\n",
    "...\n",
    "1,5,5\n",
    "1,5,7\n",
    "2,2,2\n",
    "...\n",
    "2,2,7\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58837607-c36b-426e-addd-afa3b2c8e50e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Zustandskodierung und Q-Table Datenstruktur\n",
    "Als Ergebnis der Vorüberlegung wird zugunsten der Flexibilität realitätsgetreu folgende Zustandskodierung und Q-Table Datenstruktur eingesetzt.\n",
    "\n",
    "Die **Q-Table Datenstruktur** ist ein Dictionary bestehend aus den aufgelisteten Paaren von Zustand+Aktion und deren Q-Value. Die **Zustände** sind wiederum Arrays mit den Stapelgrößen. **Aktionen** (Zustandsüberführungen) sind Tupel mit Stapelindex und Anzahl der zu entnehmenden Gegenstände. \n",
    "\n",
    "**Beispiele:**\n",
    "```\n",
    "Level 1 Beispiel:\n",
    "Q = {\n",
    "    (state,action) : Q-value\n",
    "    ...\n",
    "    }\n",
    "```\n",
    "\n",
    "``` \n",
    "Level 2 Beispiel:\n",
    "\n",
    "Q = {\n",
    "    ([3,5,7], (0,1)) : 0\n",
    "    ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71cb0b-cebd-412e-b950-769e397402a5",
   "metadata": {},
   "source": [
    "### Programm\n",
    "Das Programm ist in Python geschrieben und erlaubt, mit variabler Stapelanzahl und -größe zu spielen.\n",
    "Demnach startet das Programm mit der Abfrage der Spielfeldkonfiguration, auf der die AI dann trainiert wird. Gespielt wird im Modus Mensch gegen AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217a351-1a06-4933-a2da-81c156a534fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Konfiguration des Spielfelds\n",
    "Hier werden die Stapelgrößen festgelegt, um das Ausgangsspielfeld zu bestimmen, auf dem die AI trainiert werden soll. Die Stapel werden sortiert und die Anzahl der möglichen Zustände dieser Konfiguration berechnet und ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a829ed-e277-4acf-9692-44356b1aa09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Geben Sie die gewünschte Hölzchenanzahl pro Stapel (per Leerzeichen getrennt) ein (Beispiel: 3 5 7):  3 5 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ihre unsortierte Eingabe: [3, 5, 7]\n",
      "Sortiertes Spielfeld: [3 5 7]\n",
      "--------------------------------------------\n",
      "Spielfeld:\n",
      "Stapel 0: 3\t I I I\n",
      "Stapel 1: 5\t I I I I I\n",
      "Stapel 2: 7\t I I I I I I I\n",
      "Mögliche Zustände: 192\n"
     ]
    }
   ],
   "source": [
    "# Konfiguriere das Spielfeld/Board\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def configure_game():\n",
    "    #validate input\n",
    "    isnumeric = False\n",
    "    while not isnumeric:\n",
    "        isnumeric = True #Do-While simulation\n",
    "        \n",
    "        pileQuantities = input(\"Geben Sie die gewünschte Stapelgröße je Stapel (per Leerzeichen getrennt) ein (Beispiel: 3 5 7): \")\n",
    "\n",
    "        for x in pileQuantities.strip().split():\n",
    "            if not x.isnumeric():\n",
    "                isnumeric = False\n",
    "                print(str(x) + \" ist keine gültige Eingabe. Bitte nur Zahlen eingeben\")\n",
    "                break\n",
    "    \n",
    "    board = [int(x) for x in pileQuantities.split()]\n",
    "    print(\"Ihre unsortierte Eingabe: \" + str(board))\n",
    "    board = np.sort(board)\n",
    "    print(\"Sortiertes Spielfeld: \" + str(board))\n",
    "    return board\n",
    "        \n",
    "    \n",
    "def draw_board(state):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Spielfeld:\")\n",
    "    for row in range(len(state)):\n",
    "        out = \"Stapel \" + str(row) + \": \" + str(state[row]) +  \"\\t\"\n",
    "        for i in range(state[row]):\n",
    "            out += \" I\"\n",
    "        print(out)\n",
    "\n",
    "    \n",
    "startboard = configure_game()\n",
    "draw_board(startboard)\n",
    "\n",
    "num_States = 1\n",
    "for pile in range(len(startboard)):\n",
    "    num_States *= startboard[pile]+1\n",
    "print(\"Anzahl möglicher Zustände: \" + str(num_States))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28071aba-4245-4612-862b-ca24a1d445c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Definition der Klassen und Methoden für Nim und die AI, sowie das Trainieren und Spielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59171c1e-54c2-4c86-9ff3-eeb1dec77e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Spiel-Klasse\n",
    "class Nim():\n",
    "    \n",
    "    # Initialisiere Spielfeld (board)\n",
    "    def __init__(self, startboard=[3, 5, 7]):\n",
    "        self.piles = startboard.copy()\n",
    "        self.player = 0    # bestimmt, wer dran ist (0 or 1)\n",
    "        self.winner = None\n",
    "\n",
    "    # Gibt alle möglichen Aktionen für die übergebene Stapelkonfiguration als Tupel aus Stapelindex und zu-entnehmender-Anzahl zurück\n",
    "    @classmethod\n",
    "    def available_actions(cls, piles):\n",
    "        actions = set()\n",
    "        for i, pile in enumerate(piles):\n",
    "            for j in range(1, pile + 1):\n",
    "                actions.add((i, j))\n",
    "        return actions\n",
    "\n",
    "    # Gibt den aktuell nicht aktiven Spieler aus\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    # Wer dran wechselt auf inaktiven Spieler\n",
    "    def switch_player(self):\n",
    "        self.player = Nim.other_player(self.player)\n",
    "\n",
    "    # Führt eine Aktion (Stapel, Menge) auf dem aktuellen Spielfeld durch\n",
    "    def move(self, action):\n",
    "        pile, count = action\n",
    "\n",
    "        # Validieren\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Spiel ist bereits gewonnen\")\n",
    "        elif pile < 0 or pile >= len(self.piles):\n",
    "            raise Exception(\"Ungültiger Stapelindex\")\n",
    "        elif count < 1 or count > self.piles[pile]:\n",
    "            raise Exception(\"Ungültige Menge im Stapel\")\n",
    "\n",
    "        # Update pile\n",
    "        self.piles[pile] -= count\n",
    "        \n",
    "        # Zug beendet, anderer Spieler ist dran\n",
    "        self.switch_player()\n",
    "        \n",
    "        # Prüfe Spielende. Sieger ist gewechselter Spieler, wegen misère Variante\n",
    "        if all(pile == 0 for pile in self.piles):\n",
    "            self.winner = self.player\n",
    "        \n",
    "        \n",
    "# Lerner bzw. Computergegner\n",
    "class NimAI():\n",
    "\n",
    "    # Initialisiere AI mit leerem Q-Table\n",
    "    # Gib Lernrate, Discountfaktor und Epsilonrate als Lernparameter vor\n",
    "    # Das Q-Learning Dictionary mapt (state, action) Paare zu Q-Value\n",
    "    # state ist ein Tupel mit Stapelkonfiguration (3, 5, 7)\n",
    "    # action ist ein Tupel für (Stapelindex, Anzahl)\n",
    "    def __init__(self, learningrate=0.1, discount=0.8, epsilon=0.1):\n",
    "        self.q = dict()\n",
    "        self.learningrate = learningrate\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Aktualisiert Q-Table mithilfe von altem Zustand, der daraus verwendeten Aktion und dem\n",
    "    # resultierenden Zustand, sowie dessen Reward\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        old_q = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old_q, reward, best_future)\n",
    "\n",
    "    # Gibt Q-Value für den Zustand und die Aktion aus, falls vorhanden, sonst 0\n",
    "    def get_q_value(self, state, action):\n",
    "        # Bedenke, dass der Zustand eine Liste ist!\n",
    "        if not self.q:  # empty dict\n",
    "            return 0\n",
    "\n",
    "        Q = 0\n",
    "        if (tuple(state), action) in self.q.keys():\n",
    "            Q = self.q[(tuple(state), action)]\n",
    "        return Q\n",
    "    \n",
    "    # Q-Matrix updaten mit Formel:\n",
    "    # Q[state, action] = old_q + learningrate * deltaQ\n",
    "    # Wobei deltaQ = reward + discount * best_future_reward - old_q\n",
    "    def update_q_value(self, state, action, old_q, reward, best_future_reward):\n",
    "        deltaQ = reward + self.discount * best_future_reward - old_q\n",
    "        Q = old_q +  self.learningrate * deltaQ\n",
    "        self.q[(tuple(state), action)] = Q\n",
    "\n",
    "    # Gibt für gegebenen Zustand den Q-Value der besten Aktion aus\n",
    "    def best_future_reward(self, state):\n",
    "        #print(\"\\tbest_future_reward calls actions_dict mit state = \" + str(state))\n",
    "        actions = self.actions_dict(state)\n",
    "        if not actions:\n",
    "            return 0\n",
    "        return max(actions.values())\n",
    "\n",
    "    # Gibt für einen Zustand die zu wählende Aktion aus\n",
    "    # Während Training wird Epsilon-Exploration verwendet\n",
    "    # Beim Spielen soll die AI den besten Zug machen\n",
    "    def choose_action(self, state, epsilon=True):\n",
    "        #print(\"\\tchoose_action calls actions_dict mit state = \" + str(state))\n",
    "        actions = self.actions_dict(state)  # DICT {action:Q}\n",
    "        if not epsilon:\n",
    "            return max(actions, key=actions.get)\n",
    "        else:         \n",
    "            # Epsilon-Variante (Explore-Exploit)\n",
    "            p = np.random.random()\n",
    "            if p < self.epsilon:\n",
    "                #print(\"Epsilon obsiegte mit p = \" + str(p))\n",
    "                return list(actions.keys())[np.random.randint(len(actions.keys()))]\n",
    "            else:\n",
    "                return max(actions, key=actions.get)\n",
    "            \n",
    "            # Komplett random Variante (Exploration pur)\n",
    "            #return list(actions.keys())[np.random.randint(len(actions.keys()))]\n",
    "\n",
    "    # Gibt mögliche Aktionen als Dictionary aus\n",
    "    def actions_dict(self, state):\n",
    "        actions_set = Nim.available_actions(state)\n",
    "\n",
    "        actions = dict()  # dict[action]=Q\n",
    "        \n",
    "        for action in actions_set:\n",
    "            actions[action] = self.get_q_value(state, action)\n",
    "            \n",
    "        return actions\n",
    "\n",
    "\n",
    "# Trainiere die AI mit n Spielen gegen sich selbst\n",
    "def train(n, startboard=[3,5,7]):\n",
    "\n",
    "    player = NimAI()\n",
    "\n",
    "    # Spiele n Spiele\n",
    "    for i in range(n):\n",
    "        if i % (n/10) == 0:\n",
    "            print(f\"{int(i/n*100)}%:\\tSpiele Trainingsspiel {i}\")\n",
    "        game = Nim(startboard)\n",
    "\n",
    "        # Merke die letzten Züge beider Spieler\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "            # Speichere aktuellen Zustand und Aktion\n",
    "            state = game.piles.copy()\n",
    "            action = player.choose_action(game.piles)\n",
    "\n",
    "            # Merke die letzten Züge beider Spieler\n",
    "            last[game.player][\"state\"] = state\n",
    "            last[game.player][\"action\"] = action\n",
    "\n",
    "            # Führe Aktion aus\n",
    "            game.move(action)\n",
    "            new_state = game.piles.copy()\n",
    "\n",
    "            # When game is over, update Q values with rewards\n",
    "            # Wenn das Spiel vorbei ist, aktualisiere Q-Values\n",
    "            # Setze Rewards hier\n",
    "            if game.winner is not None:\n",
    "                player.update(state, action, new_state, -100)\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    100\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Keine neuen Rewards, solange das Spiel noch läuft\n",
    "            elif last[game.player][\"state\"] is not None:\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    print(\"100%\\tTraining abgeschlossen\")\n",
    "\n",
    "    # Gib trainierte AI aus\n",
    "    return player\n",
    "\n",
    "\n",
    "# Spiel interaktiv Mensch gegen AI\n",
    "# human_player kann 0 oder 1 gesetzt werden, um Zugreihenfolge festzulegen, sonst zufällig\n",
    "def play(ai, startboard=[3,5,7], human_player=None):\n",
    "\n",
    "    # Ohne Reihenfolge: Zufälliger Start\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Erstelle neues Spiel\n",
    "    game = Nim(startboard)\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Zeichne Spielfeld\n",
    "        print()\n",
    "        print(\"Spielfeld:\")\n",
    "        for i, pile in enumerate(game.piles):\n",
    "            print(f\"Stapel {i}: {pile}\")\n",
    "        print()\n",
    "\n",
    "        # Berechne mögliche Aktionen für Input-Validierung\n",
    "        available_actions = Nim.available_actions(game.piles)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Zeige Aktionen mit Bewertungen\n",
    "        print(\"Q-Values\")\n",
    "        for action in sorted(ai.actions_dict(game.piles)) :  # TODO sortiere Liste, für bessere Lesbarkeit\n",
    "            print(\"\\tAktion: \" + str(action) + \" Bewertung: \" + str(round(ai.get_q_value(game.piles,action),2)))\n",
    "        # Alternativ (ohne Umbrüche aber direkter)\n",
    "        #print(str(ai.actions_dict(game.piles)))\n",
    "\n",
    "        # Mensch am Zug\n",
    "        if game.player == human_player:\n",
    "            print(\"Dein Zug\")\n",
    "            while True:\n",
    "                pile = int(input(\"Wähle Stapel: \"))\n",
    "                count = int(input(\"Wähle Anzahl: \"))\n",
    "                if (pile, count) in available_actions:\n",
    "                    break\n",
    "                print(\"Ungültige Aktion, versuch es nochmal.\")\n",
    "\n",
    "        # Computer am Zug\n",
    "        else:\n",
    "            print(\"AI's Zug\")\n",
    "            pile, count = ai.choose_action(game.piles, epsilon=False) # epsilon false, damit bester Zug gewählt wird\n",
    "            print(f\"AI nahm {count} von Stapel {pile}.\")\n",
    "\n",
    "        # Make move\n",
    "        game.move((pile, count))\n",
    "\n",
    "        # Check for winner\n",
    "        if game.winner is not None:\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            winner = \"Mensch\" if game.winner == human_player else \"AI\"\n",
    "            print(f\"{winner} hat gewonnen\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910eb644-4bdb-42f5-a49d-1df72d57305c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training der AI\n",
    "Trainiert die AI in so vielen Epochen, wie angegeben wird und gibt dann die gesamte Q-Table aus (Ausgabe ggfs. entkommentieren)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee13ea5-cfb7-4012-8805-4d2fb764d746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%:\tSpiele Trainingsspiel 0\n",
      "10%:\tSpiele Trainingsspiel 10000\n",
      "20%:\tSpiele Trainingsspiel 20000\n",
      "30%:\tSpiele Trainingsspiel 30000\n",
      "40%:\tSpiele Trainingsspiel 40000\n",
      "50%:\tSpiele Trainingsspiel 50000\n",
      "60%:\tSpiele Trainingsspiel 60000\n",
      "70%:\tSpiele Trainingsspiel 70000\n",
      "80%:\tSpiele Trainingsspiel 80000\n",
      "90%:\tSpiele Trainingsspiel 90000\n",
      "100%\tTraining abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# Trainiere das Modell \n",
    "ai = train(100000, startboard)\n",
    "\n",
    "# Zeichne Q-Table\n",
    "# for entry in sorted(ai.q, reverse=True):\n",
    "#     print(\"(State,Action): \" + str(entry) + \"\\tBewertung: \" + str(round(ai.get_q_value(entry[0],entry[1]),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e01fd-1f73-4be2-8998-81b25b912862",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Spielen\n",
    "Startet das Spiel gegen den Computer. Durch optionale Angabe von 0 beginnt der Mensch, bei 1 der Computer. Ohne diese Festlegung wird der Startspieler zufällig bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd81cc3e-cdfa-473e-87cf-3e0787a54059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 1\n",
      "Stapel 1: 2\n",
      "Stapel 2: 3\n",
      "Stapel 3: 4\n",
      "Stapel 4: 5\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (0, 1) Bewertung: 31.15\n",
      "\tAktion: (1, 1) Bewertung: -1.47\n",
      "\tAktion: (1, 2) Bewertung: -9.57\n",
      "\tAktion: (2, 1) Bewertung: 27.63\n",
      "\tAktion: (2, 2) Bewertung: -5.39\n",
      "\tAktion: (2, 3) Bewertung: 3.25\n",
      "\tAktion: (3, 1) Bewertung: -5.1\n",
      "\tAktion: (3, 2) Bewertung: -15.32\n",
      "\tAktion: (3, 3) Bewertung: -13.65\n",
      "\tAktion: (3, 4) Bewertung: -26.05\n",
      "\tAktion: (4, 1) Bewertung: 27.81\n",
      "\tAktion: (4, 2) Bewertung: -2.83\n",
      "\tAktion: (4, 3) Bewertung: -15.89\n",
      "\tAktion: (4, 4) Bewertung: -22.85\n",
      "\tAktion: (4, 5) Bewertung: -19.52\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  0\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 3\n",
      "Stapel 3: 4\n",
      "Stapel 4: 5\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -12.24\n",
      "\tAktion: (1, 2) Bewertung: -12.84\n",
      "\tAktion: (2, 1) Bewertung: -7.36\n",
      "\tAktion: (2, 2) Bewertung: -10.39\n",
      "\tAktion: (2, 3) Bewertung: -10.92\n",
      "\tAktion: (3, 1) Bewertung: -5.65\n",
      "\tAktion: (3, 2) Bewertung: -9.12\n",
      "\tAktion: (3, 3) Bewertung: -35.27\n",
      "\tAktion: (3, 4) Bewertung: -32.47\n",
      "\tAktion: (4, 1) Bewertung: -3.94\n",
      "\tAktion: (4, 2) Bewertung: -14.72\n",
      "\tAktion: (4, 3) Bewertung: -14.77\n",
      "\tAktion: (4, 4) Bewertung: -21.86\n",
      "\tAktion: (4, 5) Bewertung: -23.81\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 4.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 3\n",
      "Stapel 3: 4\n",
      "Stapel 4: 4\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -10.17\n",
      "\tAktion: (1, 2) Bewertung: -22.08\n",
      "\tAktion: (2, 1) Bewertung: 38.74\n",
      "\tAktion: (2, 2) Bewertung: -0.07\n",
      "\tAktion: (2, 3) Bewertung: -12.56\n",
      "\tAktion: (3, 1) Bewertung: -9.86\n",
      "\tAktion: (3, 2) Bewertung: -14.2\n",
      "\tAktion: (3, 3) Bewertung: -26.46\n",
      "\tAktion: (3, 4) Bewertung: -26.95\n",
      "\tAktion: (4, 1) Bewertung: -8.71\n",
      "\tAktion: (4, 2) Bewertung: -13.45\n",
      "\tAktion: (4, 3) Bewertung: -38.02\n",
      "\tAktion: (4, 4) Bewertung: -21.65\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  2\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 4\n",
      "Stapel 4: 4\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -11.02\n",
      "\tAktion: (1, 2) Bewertung: -18.78\n",
      "\tAktion: (2, 1) Bewertung: -16.49\n",
      "\tAktion: (2, 2) Bewertung: -20.39\n",
      "\tAktion: (3, 1) Bewertung: -15.95\n",
      "\tAktion: (3, 2) Bewertung: -22.45\n",
      "\tAktion: (3, 3) Bewertung: -38.73\n",
      "\tAktion: (3, 4) Bewertung: -35.29\n",
      "\tAktion: (4, 1) Bewertung: -10.71\n",
      "\tAktion: (4, 2) Bewertung: -17.34\n",
      "\tAktion: (4, 3) Bewertung: -26.65\n",
      "\tAktion: (4, 4) Bewertung: -42.12\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 4.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 4\n",
      "Stapel 4: 3\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -27.86\n",
      "\tAktion: (1, 2) Bewertung: -36.0\n",
      "\tAktion: (2, 1) Bewertung: -31.26\n",
      "\tAktion: (2, 2) Bewertung: -38.91\n",
      "\tAktion: (3, 1) Bewertung: 44.14\n",
      "\tAktion: (3, 2) Bewertung: -23.68\n",
      "\tAktion: (3, 3) Bewertung: -27.1\n",
      "\tAktion: (3, 4) Bewertung: -31.83\n",
      "\tAktion: (4, 1) Bewertung: -20.01\n",
      "\tAktion: (4, 2) Bewertung: -31.94\n",
      "\tAktion: (4, 3) Bewertung: -40.47\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  3\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 3\n",
      "Stapel 4: 3\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -40.87\n",
      "\tAktion: (1, 2) Bewertung: -29.59\n",
      "\tAktion: (2, 1) Bewertung: -30.29\n",
      "\tAktion: (2, 2) Bewertung: -30.11\n",
      "\tAktion: (3, 1) Bewertung: -28.69\n",
      "\tAktion: (3, 2) Bewertung: -36.64\n",
      "\tAktion: (3, 3) Bewertung: -36.11\n",
      "\tAktion: (4, 1) Bewertung: -15.67\n",
      "\tAktion: (4, 2) Bewertung: -29.96\n",
      "\tAktion: (4, 3) Bewertung: -36.93\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 4.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 3\n",
      "Stapel 4: 2\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -32.19\n",
      "\tAktion: (1, 2) Bewertung: -32.17\n",
      "\tAktion: (2, 1) Bewertung: -32.36\n",
      "\tAktion: (2, 2) Bewertung: -35.55\n",
      "\tAktion: (3, 1) Bewertung: 51.77\n",
      "\tAktion: (3, 2) Bewertung: -27.05\n",
      "\tAktion: (3, 3) Bewertung: -40.49\n",
      "\tAktion: (4, 1) Bewertung: -36.04\n",
      "\tAktion: (4, 2) Bewertung: -38.07\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  3\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 2\n",
      "Stapel 4: 2\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -43.57\n",
      "\tAktion: (1, 2) Bewertung: -49.16\n",
      "\tAktion: (2, 1) Bewertung: -43.2\n",
      "\tAktion: (2, 2) Bewertung: -53.67\n",
      "\tAktion: (3, 1) Bewertung: -43.62\n",
      "\tAktion: (3, 2) Bewertung: -57.79\n",
      "\tAktion: (4, 1) Bewertung: -14.14\n",
      "\tAktion: (4, 2) Bewertung: -49.94\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 4.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 2\n",
      "Stapel 4: 1\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: 63.99\n",
      "\tAktion: (1, 2) Bewertung: -44.16\n",
      "\tAktion: (2, 1) Bewertung: 64.0\n",
      "\tAktion: (2, 2) Bewertung: -36.68\n",
      "\tAktion: (3, 1) Bewertung: 63.98\n",
      "\tAktion: (3, 2) Bewertung: -55.94\n",
      "\tAktion: (4, 1) Bewertung: -49.72\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  3\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 1\n",
      "Stapel 4: 1\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -60.93\n",
      "\tAktion: (1, 2) Bewertung: -62.89\n",
      "\tAktion: (2, 1) Bewertung: -63.62\n",
      "\tAktion: (2, 2) Bewertung: -62.56\n",
      "\tAktion: (3, 1) Bewertung: -54.53\n",
      "\tAktion: (4, 1) Bewertung: -59.58\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 3.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 0\n",
      "Stapel 4: 1\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -51.32\n",
      "\tAktion: (1, 2) Bewertung: -68.44\n",
      "\tAktion: (2, 1) Bewertung: -62.06\n",
      "\tAktion: (2, 2) Bewertung: -45.71\n",
      "\tAktion: (4, 1) Bewertung: 80.0\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  4\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 2\n",
      "Stapel 3: 0\n",
      "Stapel 4: 0\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -75.41\n",
      "\tAktion: (1, 2) Bewertung: -79.81\n",
      "\tAktion: (2, 1) Bewertung: -79.73\n",
      "\tAktion: (2, 2) Bewertung: -68.2\n",
      "AI's Zug\n",
      "AI nahm 2 von Stapel 2.\n",
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 2\n",
      "Stapel 2: 0\n",
      "Stapel 3: 0\n",
      "Stapel 4: 0\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: 100.0\n",
      "\tAktion: (1, 2) Bewertung: -100.0\n",
      "Dein Zug\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wähle Stapel:  1\n",
      "Wähle Anzahl:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spielfeld:\n",
      "Stapel 0: 0\n",
      "Stapel 1: 1\n",
      "Stapel 2: 0\n",
      "Stapel 3: 0\n",
      "Stapel 4: 0\n",
      "\n",
      "Q-Values\n",
      "\tAktion: (1, 1) Bewertung: -100.0\n",
      "AI's Zug\n",
      "AI nahm 1 von Stapel 1.\n",
      "\n",
      "GAME OVER\n",
      "Mensch hat gewonnen\n"
     ]
    }
   ],
   "source": [
    "#Spiele gegen den Computer\n",
    "play(ai, startboard, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58959e-3aa7-40a2-b2ab-0013f9a74d9e",
   "metadata": {},
   "source": [
    "## Bewertung\n",
    "Wie schlägt sich die AI schließlich: Gewinnt sie gegen einen Menschen? Gewinnt sie gegen die Gewinnstrategie nach Scam Nation?\n",
    "\n",
    "Zunächst wurde die AI mit purer Exploration trainiert, was zur Folge hatte, dass sie die Pole Positions nicht verwendet hat. Sie war in etwa so gut, wie es ein Mensch ohne Gewinnstrategie auch wäre. Zum Ende des Spiels waren die Entscheidungen der AI hochwertig und zielsicher, zu Beginn eher weniger.\n",
    "\n",
    "Mit der Einführung der Epsilon-Exploration wurde die AI regelrecht zum Nim-Endgegner. Gnadenlos zwingt sie ihre Kontrahenten in ungewinnbare Spielsituationen. Dabei verfolgt sie exakt die bekannte Gewinnstrategie mit den Pole Positions. Wenn die AI zuerst zieht, nimmt sie immer nur ein Objekt, was dem Gegenspieler die Pole Positions verwehrt. Falls der Mensch zuerst zieht und somit im dritten Zug die Pole Positions für sich gewinnt, macht die AI extra *kleine* Züge, in denen sie nur ein Objekt nimmt, was das Spiel maximal lang macht. Sie maximiert damit die Anzahl der Züge, die der Gegenspieler machen muss, welche stets ein (geringes) Risiko für Fehler bieten. Das passiert entweder unabsichtlich und entsteht durch die Sortierung der möglichen Aktionen im Programmcode, von denen die AI bei gleicher Bewertung einfach die erste nimmt oder durch zufällige Exploration des Gegenspielers beim Training: Wenn im Training der Computer gegen sich selbst spielt, kann es in 10% (Epsilonfaktor) der Fälle dazu kommen, dass ein Spieler, der die Siegerstellung der Pole Positions besitzt, eine zufällige Aktion durchführt, was ihn den Sieg kosten kann. Wie bereits im Kapitel der Spielregeln beschrieben, bietet genau das auch beim Spiel gegen/zwischen Menschen stets eine Siegeschance durch menschliches Versagen. An dieser Stelle bildet der Q-Learning Algorithmus mit Exploration und Exploitation also bemerkenswert detailgetreu die Realität ab.\n",
    "\n",
    "Für das Standardspiel [3 5 7] ist die Gewinnstrategie für den Menschen nun klar und je nach Zug-Reihenfolge kann ein Mensch die AI besiegen. Beim Spiel mit anderen Spielfeldkonfigurationen (zum Beispiel [1 2 3 4 5] oder [2 6 7 8]), greift die bekannte Strategie jedoch nicht mehr unbedingt und hier zeigt die AI ihr wahres Potential. Sie ist in der Lage für jede erdenkliche Spielfeld-Variante eine Strategie zu entwickeln und hängt den Menschen siegessicher ab. Es lässt sich erkennen, dass sie dabei das Problem auf die bekannten Pole Positions reduziert. Diese Strategie könnte ein Mensch bei einer neuen Spielfeldkonfiguration ebenfalls verfolgen, nur ist der Weg zu den bekannten Schlüsselpositionen für den Menschen nicht mehr so offensichtlich wie zuvor. Die Strategie der AI hingegen umfasst bereits weitere (neue) Pole Positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce709ee7-e9ac-4c8d-a21a-9bea33a9518f",
   "metadata": {},
   "source": [
    "## Herausforderungen\n",
    "Die Datenstruktur der Q-Table und der Zugriff auf dessen Einträge über die Zustandskodierung als Arrays, erweiste sich als überaus schwer begreiflich. Diese Form wurde im vorangehenden Unterricht nicht behandelt und musste *explorativ* implementiert werden, anstatt, dass bereits behandelte Datenstrukturen *exploitet* werden konnten.\n",
    "\n",
    "Die immense positive Wirkung der Epsilon-Exploration auf die Fähigkeit der AI ist nicht offensichtlich gewesen. Das etwas zufällig wirkende Verhalten der AI zu Beginn der Spiele wurde beinahe als Endresultat akzeptiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b4483-6347-4c10-ae9f-de01caba79c3",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "Mithilfe des Q-Learning Algorithmus konnte eine AI entwickelt werden, die das Nim-Spiel mit beliebiger Spielfeldkonfiguration **beherrscht**. Dabei konnte eine sinnvolle und **flexible** Zustandskodierung mit Arrays durchgesetzt werden.\n",
    "\n",
    "Dem menschlichen Spieler ist es möglich, eine Spielfeldkonfiguration vorzugeben, auf der eine AI trainiert wird, die analog zu der *Gewinnstrategie nach Scam Nation* agiert. Beim Spielen gegen die AI kann der Mensch seine Fähigkeiten auf die Probe stellen oder auf die Informationen der Q-Table zurückgreifen. Durch einen Blick auf die Q-Values der möglichen Aktionen kann oft schon nach wenigen Zügen festgestellt werden, dass das Spiel bereits entschieden ist. Ein Mensch kann kaum darauf hoffen, dass die AI in einer echten Partie ihre Siegerstellung durch einen Fehler aufgibt, wie es womöglich ein Mensch mit jedem Zug riskiert.\n",
    "\n",
    "Als abschließende Note lässt sich für diesen Fall festhalten:\n",
    "**Exploitation ist wichtig. Lernt die AI *random*, spielt sie auch eher *random*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f05902-e857-44d1-8965-01c2db6b2f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
